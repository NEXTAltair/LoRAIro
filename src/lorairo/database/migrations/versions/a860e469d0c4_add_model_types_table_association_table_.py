"""Add model_types table, association table, discontinued_at to models, and setup relationships

Revision ID: a860e469d0c4
Revises: 879cc87e4125
Create Date: 2025-04-16 16:47:55.241968

"""

import logging  # For logging warnings
from collections.abc import Sequence

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision: str = "a860e469d0c4"
down_revision: str | None = "879cc87e4125"
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None

# Setup logger for migration warnings/info
log = logging.getLogger(f"alembic.runtime.migration.{revision}")

# --- New models to add ---
# Grouped by their intended type for easier insertion logic
NEW_MODELS_DATA = {
    "score": [
        "aesthetic_shadow_v1",
        "aesthetic_shadow_v2",
        "cafe_aesthetic",
        "ImprovedAesthetic",
        "WaifuAesthetic",
    ],
    "tagger": [
        "idolsankaku-eva02-large-tagger-v1",
        "idolsankaku-swinv2-tagger-v1",
        "Z3D-E621-Convnext",
        "wd-v1-4-convnext-tagger-v2",
        "wd-v1-4-convnextv2-tagger-v2",
        "wd-v1-4-moat-tagger-v2",
        "wd-v1-4-swinv2-tagger-v2",
        "wd-vit-tagger-v3",
        "wd-convnext-tagger-v3",
        "wd-swinv2-tagger-v3",
        "wd-vit-large-tagger-v3",
        "wd-eva02-large-tagger-v3",
        "deepdanbooru-v3-20211112-sgd-e28",
        "deepdanbooru-v4-20200814-sgd-e30",
        "deepdanbooru-v3-20200915-sgd-e30",
        "deepdanbooru-v3-20200101-sgd-e30",
        "deepdanbooru-v1-20191108-sgd-e30",
    ],
    "captioner": [
        "BLIPLargeCaptioning",
        "blip2-opt-2.7b",
        "blip2-opt-2.7b-coco",
        "blip2-opt-6.7b",
        "blip2-opt-6.7b-coco",
        "blip2-flan-t5-xl",
        "blip2-flan-t5-xl-coco",
        "blip2-flan-t5-xxl",
        "GITLargeCaptioning",
    ],
    "llm": [
        "ToriiGate-v0.3",
        "gemini-1.5-pro",
        "gemini-1.5-flash",
        "gemini-2.0-flash",
        "gpt-4o-mini",
        "gpt-4o",
        "gpt-4.5-preview",
        "optimus-alpha",
        "claude-3-7-sonnet",
        "claude-3-5-haiku",
    ],
    "upscaler": [],  # No new upscalers in the list
}


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###

    # 1. Create model_types table
    model_types_table = op.create_table(
        "model_types",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("name", sa.String(), nullable=False),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("name"),
    )

    # 2. Insert initial data into model_types
    initial_types = [
        {"name": "tagger"},
        {"name": "score"},
        {"name": "captioner"},
        {"name": "upscaler"},
        {"name": "llm"},
    ]
    op.bulk_insert(model_types_table, initial_types)

    # 3. Create association table
    model_function_associations_table = op.create_table(
        "model_function_associations",
        sa.Column("model_id", sa.Integer(), nullable=False),
        sa.Column("type_id", sa.Integer(), nullable=False),
        sa.ForeignKeyConstraint(
            ["model_id"],
            ["models.id"],
            name="fk_assoc_model_id",  # Naming constraint
        ),
        sa.ForeignKeyConstraint(
            ["type_id"],
            ["model_types.id"],
            name="fk_assoc_type_id",  # Naming constraint
        ),
        sa.PrimaryKeyConstraint("model_id", "type_id", name="pk_assoc"),  # Naming constraint
    )

    # 4. Add discontinued_at column to models (BEFORE data migration needs old 'type')
    with op.batch_alter_table("models", schema=None) as batch_op:
        batch_op.add_column(sa.Column("discontinued_at", sa.TIMESTAMP(timezone=True), nullable=True))

    # 5. Make the old 'type' column nullable temporarily BEFORE inserting new models
    with op.batch_alter_table("models", schema=None) as batch_op:
        batch_op.alter_column("type", existing_type=sa.String(), nullable=True)
        log.info("Temporarily made 'type' column nullable in models table.")

    # --- ここから新規モデル登録と既存データ移行処理 ---
    bind = op.get_bind()
    Session = sa.orm.sessionmaker(bind=bind)
    session = Session()

    # Define table structures for query
    models_table = sa.table(
        "models",
        sa.column("id", sa.Integer),
        sa.column("name", sa.String),
        sa.column("type", sa.String),  # For reading existing data
        # Add other columns needed for INSERT if defaults are not sufficient
        # sa.column("provider", sa.String),
        # sa.column("created_at", sa.TIMESTAMP(timezone=True)),
        # sa.column("updated_at", sa.TIMESTAMP(timezone=True)),
    )
    # model_types_table is already defined
    # model_function_associations_table is already defined

    try:
        # Get new type names and their IDs (already inserted)
        model_types_map = {
            row.name: row.id
            for row in session.execute(
                sa.select(model_types_table.c.id, model_types_table.c.name)
            ).fetchall()
        }
        log.info(f"Model types map loaded: {model_types_map}")

        # --- 4.1. 新規モデルの登録 ---
        new_models_to_insert = []
        new_associations_to_insert = []
        inserted_model_names = set()

        # Fetch existing model names to avoid duplicates
        existing_model_names = {
            row.name for row in session.execute(sa.select(models_table.c.name)).fetchall()
        }
        log.info(f"Found {len(existing_model_names)} existing models.")

        for type_name, model_list in NEW_MODELS_DATA.items():
            if type_name not in model_types_map:
                log.warning(
                    f"新しいモデルのタイプ '{type_name}' が model_types テーブルに存在しません。スキップします。"
                )
                continue

            type_id = model_types_map[type_name]
            for model_name in model_list:
                if model_name not in existing_model_names and model_name not in inserted_model_names:
                    # Prepare model data for insertion (only name is mandatory here)
                    new_models_to_insert.append({"name": model_name})
                    inserted_model_names.add(model_name)  # Track insertions in this run
                elif model_name in existing_model_names:
                    log.warning(
                        f"新規追加予定のモデル '{model_name}' は既に存在します。挿入をスキップします。"
                    )

        if new_models_to_insert:
            log.info(f"Inserting {len(new_models_to_insert)} new models...")
            # Use bulk_insert with 'returning' to get IDs might be DB specific.
            # Safer to insert then query back.
            op.bulk_insert(models_table, new_models_to_insert)
            session.flush()  # Ensure inserts are processed before querying

            # Query back the newly inserted models to get their IDs
            newly_inserted_models = session.execute(
                sa.select(models_table.c.id, models_table.c.name).where(
                    models_table.c.name.in_(inserted_model_names)
                )
            ).fetchall()
            new_model_id_map = {model.name: model.id for model in newly_inserted_models}
            log.info(f"Retrieved IDs for {len(new_model_id_map)} newly inserted models.")

            # Prepare associations for the newly inserted models
            for type_name, model_list in NEW_MODELS_DATA.items():
                if type_name not in model_types_map:
                    continue
                type_id = model_types_map[type_name]
                for model_name in model_list:
                    if model_name in new_model_id_map:
                        new_associations_to_insert.append(
                            {"model_id": new_model_id_map[model_name], "type_id": type_id}
                        )

            if new_associations_to_insert:
                log.info(f"Preparing {len(new_associations_to_insert)} associations for new models.")

        else:
            log.info("追加する新しいモデルはありませんでした。")

        # --- 4.2. 既存モデルのデータ移行 ---
        existing_associations_to_insert = []
        log.info("Migrating existing models...")
        existing_models = session.execute(
            sa.select(models_table.c.id, models_table.c.name, models_table.c.type)
        ).fetchall()

        for model_id, model_name, old_type in existing_models:
            new_type_name = None
            if old_type == "upscaler":
                new_type_name = "upscaler"
            elif old_type == "score":
                new_type_name = "score"
            elif old_type == "vision":
                # Check if this existing 'vision' model is actually one of the new ones
                # This handles the case where a model existed with 'vision' type
                # but should now be classified differently based on the NEW_MODELS_DATA
                mapped = False
                for n_type, n_list in NEW_MODELS_DATA.items():
                    if model_name in n_list:
                        new_type_name = n_type
                        mapped = True
                        break
                if not mapped:  # If not found in new lists, default 'vision' maps to 'llm'
                    new_type_name = "llm"

            else:
                log.warning(
                    f"既存モデルID {model_id} ('{model_name}') の旧タイプ '{old_type}' に対する移行ルールが未定義です。スキップします。"
                )

            if new_type_name and new_type_name in model_types_map:
                # Avoid inserting duplicate association if model was already in NEW_MODELS_DATA and processed
                if model_name not in new_model_id_map:  # Only add if it wasn't a "newly inserted" one
                    existing_associations_to_insert.append(
                        {"model_id": model_id, "type_id": model_types_map[new_type_name]}
                    )
            elif new_type_name:
                log.warning(
                    f"既存モデルID {model_id} ('{model_name}') の新タイプ '{new_type_name}' が model_types テーブルに存在しません。スキップします。"
                )
            # else: # Warning already printed above if rule is undefined

        log.info(f"Preparing {len(existing_associations_to_insert)} associations for existing models.")

        # Combine and insert all associations
        all_associations = new_associations_to_insert + existing_associations_to_insert
        if all_associations:
            log.info(f"Inserting {len(all_associations)} total associations...")
            op.bulk_insert(model_function_associations_table, all_associations)
        else:
            log.info("挿入する関連付けはありませんでした。")

        session.commit()  # Commit the transaction for data migration queries
    except Exception as e:
        session.rollback()
        log.error(f"データ移行中にエラーが発生しました: {e}", exc_info=True)
        raise  # Re-raise the exception to halt the migration
    finally:
        session.close()
    # --- データ移行処理ここまで ---

    # 5. Drop the old 'type' column AFTER data migration
    with op.batch_alter_table("models", schema=None) as batch_op:
        batch_op.drop_column("type")
        log.info("Dropped 'type' column from models table.")

    # --- Other schema adjustments ---
    with op.batch_alter_table("ratings", schema=None) as batch_op:
        # try:
        #     # Try dropping constraint - この行を削除またはコメントアウト
        #     # batch_op.drop_constraint("fk_ratings_model_id_models", type_="foreignkey")
        # except Exception:
        #     log.warning(
        #         "Could not drop constraint 'fk_ratings_model_id_models' on ratings (may not exist)."
        #     )
        # 目的の制約を確実に作成する
        batch_op.create_foreign_key(
            "fk_ratings_model_id_models", "models", ["model_id"], ["id"], ondelete="CASCADE"
        )
        log.info(
            "Ensured 'fk_ratings_model_id_models' foreign key with ON DELETE CASCADE exists on ratings table."
        )

    with op.batch_alter_table("scores", schema=None) as batch_op:
        batch_op.alter_column(
            "is_edited_manually",
            existing_type=sa.BOOLEAN(),
            nullable=True,
            # existing_server_default=sa.text("0"), # Removing default as per previous script
        )

    log.info("Upgrade script finished.")
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    log.info("Starting downgrade script...")
    # ### commands auto generated by Alembic - please adjust! ###

    # Add 'type' column first, nullable initially
    with op.batch_alter_table("models", schema=None) as batch_op:
        batch_op.add_column(sa.Column("type", sa.VARCHAR(), nullable=True))
        log.info("Added nullable 'type' column to models table.")

    # --- ここからデータ逆移行処理 ---
    bind = op.get_bind()
    Session = sa.orm.sessionmaker(bind=bind)
    session = Session()

    models_table = sa.table(
        "models", sa.column("id", sa.Integer), sa.column("type", sa.String), sa.column("name", sa.String)
    )
    model_types_table = sa.table("model_types", sa.column("id", sa.Integer), sa.column("name", sa.String))
    model_function_associations_table = sa.table(
        "model_function_associations", sa.column("model_id", sa.Integer), sa.column("type_id", sa.Integer)
    )
    # --- Define NEW_MODELS_DATA again for downgrade logic ---
    # (Copy NEW_MODELS_DATA definition here or import if moved to shared location)
    _NEW_MODELS_DATA = {
        "score": [
            "aesthetic_shadow_v1",
            "aesthetic_shadow_v2",
            "cafe_aesthetic",
            "ImprovedAesthetic",
            "WaifuAesthetic",
        ],
        "tagger": [
            "idolsankaku-eva02-large-tagger-v1",
            "idolsankaku-swinv2-tagger-v1",
            "Z3D-E621-Convnext",
            "wd-v1-4-convnext-tagger-v2",
            "wd-v1-4-convnextv2-tagger-v2",
            "wd-v1-4-moat-tagger-v2",
            "wd-v1-4-swinv2-tagger-v2",
            "wd-vit-tagger-v3",
            "wd-convnext-tagger-v3",
            "wd-swinv2-tagger-v3",
            "wd-vit-large-tagger-v3",
            "wd-eva02-large-tagger-v3",
            "deepdanbooru-v3-20211112-sgd-e28",
            "deepdanbooru-v4-20200814-sgd-e30",
            "deepdanbooru-v3-20200915-sgd-e30",
            "deepdanbooru-v3-20200101-sgd-e30",
            "deepdanbooru-v1-20191108-sgd-e30",
        ],
        "captioner": [
            "BLIPLargeCaptioning",
            "blip2-opt-2.7b",
            "blip2-opt-2.7b-coco",
            "blip2-opt-6.7b",
            "blip2-opt-6.7b-coco",
            "blip2-flan-t5-xl",
            "blip2-flan-t5-xl-coco",
            "blip2-flan-t5-xxl",
            "GITLargeCaptioning",
        ],
        "llm": [
            "ToriiGate-v0.3",
            "gemini-1.5-pro",
            "gemini-1.5-flash",
            "gemini-2.0-flash",
            "gpt-4o-mini",
            "gpt-4o",
            "gpt-4.5-preview",
            "optimus-alpha",
            "claude-3-7-sonnet",
            "claude-3-5-haiku",
        ],
        "upscaler": [],
    }
    new_model_names_set = {name for names in _NEW_MODELS_DATA.values() for name in names}

    try:
        # Get associations and type names
        query = sa.select(
            model_function_associations_table.c.model_id,
            model_types_table.c.name.label("type_name"),  # Alias to avoid potential conflicts
        ).join(model_types_table, model_function_associations_table.c.type_id == model_types_table.c.id)
        associations = session.execute(query).fetchall()

        # Map model_id to its new type (might be multiple, choose one for old 'type')
        model_id_to_new_type = {}
        for assoc in associations:
            # Prioritize simpler types for reverse mapping if multiple exist
            current_priority = 99
            if assoc.model_id not in model_id_to_new_type:
                model_id_to_new_type[assoc.model_id] = assoc.type_name
            else:
                # Simple priority: upscaler > score > tagger > captioner > llm
                priority_map = {"upscaler": 0, "score": 1, "tagger": 2, "captioner": 3, "llm": 4}
                new_priority = priority_map.get(assoc.type_name, 99)
                existing_priority = priority_map.get(model_id_to_new_type[assoc.model_id], 99)
                if new_priority < existing_priority:
                    model_id_to_new_type[assoc.model_id] = assoc.type_name

        # Update models table with old type based on new type
        models_to_update = session.execute(sa.select(models_table.c.id, models_table.c.name)).fetchall()
        log.info(f"Performing reverse migration for {len(models_to_update)} models.")

        for model_id, model_name in models_to_update:
            # Skip models that were newly added in the upgrade
            if model_name in new_model_names_set:
                continue  # These didn't exist before, so type should remain NULL or be handled differently

            new_type = model_id_to_new_type.get(model_id)
            old_type = None
            if new_type == "upscaler":
                old_type = "upscaler"
            elif new_type == "score":
                old_type = "score"
            elif new_type in ["tagger", "captioner", "llm"]:
                # All these were likely 'vision' before, or didn't exist
                old_type = "vision"  # Map back to vision as the best guess

            if old_type:
                stmt = sa.update(models_table).where(models_table.c.id == model_id).values(type=old_type)
                session.execute(stmt)
            else:
                log.warning(
                    f"モデルID {model_id} ('{model_name}') のタイプ '{new_type}' から古いタイプへの逆マッピング不明。NULLのまま。"
                )

        session.commit()
        log.info("Reverse data migration committed.")

        # Now make the column non-nullable if it was originally (assuming it was)
        with op.batch_alter_table("models", schema=None) as batch_op:
            batch_op.alter_column("type", existing_type=sa.VARCHAR(), nullable=False)
            log.info("Made 'type' column non-nullable.")

    except Exception as e:
        session.rollback()
        log.error(f"データ逆移行中にエラーが発生しました: {e}", exc_info=True)
        raise
    finally:
        session.close()

    # --- データ逆移行処理ここまで ---

    # Drop other elements in reverse order of creation
    with op.batch_alter_table("scores", schema=None) as batch_op:
        batch_op.alter_column(
            "is_edited_manually",
            existing_type=sa.BOOLEAN(),
            nullable=False,  # Restore original nullability
            # existing_server_default=sa.text("0"), # Restore default if needed
        )

    with op.batch_alter_table("ratings", schema=None) as batch_op:
        try:
            # upgrade で作成した制約を削除
            batch_op.drop_constraint("fk_ratings_model_id_models", type_="foreignkey")
            log.info("Dropped constraint 'fk_ratings_model_id_models' from ratings table.")
        except Exception:
            log.warning(
                "Could not drop constraint 'fk_ratings_model_id_models' on ratings during downgrade (may not exist)."
            )
        # 必要であれば、元の制約を再作成するコードをここに追加
        # op.create_foreign_key('元の制約名', "models", ["model_id"], ["id"], ondelete="元のルール")

    with op.batch_alter_table("models", schema=None) as batch_op:
        batch_op.drop_column("discontinued_at")
        log.info("Dropped 'discontinued_at' column from models.")

    op.drop_table("model_function_associations")
    log.info("Dropped 'model_function_associations' table.")
    op.drop_table("model_types")
    log.info("Dropped 'model_types' table.")

    log.info("Downgrade script finished.")
    # ### end Alembic commands ###
