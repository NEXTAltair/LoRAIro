# LoRAIro Technical Specification

## Technology Stack

### Core Technologies

#### Programming Language
- **Python 3.11+**: Primary development language
  - Modern type hints and features
  - Excellent AI/ML library ecosystem
  - Strong GUI framework support

#### GUI Framework
- **PySide6**: Qt for Python
  - Cross-platform desktop application framework
  - Rich widget set and layout management
  - Signal/slot event system
  - Designer-based UI development

**DirectoryPicker Widget Validation (2025/07/12 Enhancement)**
- **Purpose**: Prevent invalid directory selection that causes unintended batch processing
- **Validation Strategy**: Hierarchical depth limitation + file count control
- **Implementation**: Custom `validDirectorySelected` signal with pre-validation
- **Performance Constraints**:
  - Maximum directory depth: 3 levels
  - Maximum file scan limit: 10,000 files
  - Early termination on system directory detection
- **Validation Criteria**:
  - Directory must exist and be readable
  - Must contain at least 1 image file (.jpg, .png, .webp, .bmp)
  - File count must not exceed upper limit (prevents system directory selection)
  - Directory hierarchy must not exceed depth limit (prevents deep recursive scanning)
- **Signal Behavior**:
  - `textChanged`: Disabled for manual input validation
  - `validDirectorySelected`: Only emitted after successful validation
  - Trigger events: Enter key, focus loss, dialog selection, history selection

#### Database
- **SQLite**: Embedded database
  - Zero-configuration local storage
  - ACID compliance
  - File-based portability
- **SQLAlchemy**: Object-Relational Mapping
  - Declarative model definitions
  - Database abstraction layer
  - Query optimization
- **Alembic**: Database migration management
  - Version-controlled schema changes
  - Forward and backward migrations
  - Team collaboration support

#### Package Management
- **uv**: Modern Python package manager
  - Fast dependency resolution
  - Virtual environment management
  - Lock file for reproducible builds
  - Local package integration

### AI Integration

#### Supported AI Providers

**OpenAI GPT-4 Vision**
- Image analysis and captioning
- Structured output generation
- High-quality descriptions
- Rate limiting and quota management

**Anthropic Claude**
- Advanced image understanding
- Detailed analysis capabilities
- Context-aware descriptions
- Safety-focused outputs

**Google Gemini**
- Multi-modal AI processing
- Image and text integration
- Scalable API access
- Competitive pricing

**Local ML Models** (via image-annotator-lib)
- **CLIP**: Aesthetic scoring and similarity assessment
- **DeepDanbooru**: Anime/artwork tagging models
- **ONNX Runtime**: Cross-platform model inference
- **TensorFlow**: ML model execution framework
- **Transformers**: Hugging Face model integration

#### AI Integration Architecture

**Integration Flow**
```
WorkerService â†’ AnnotationWorker â†’ ai_annotator.py â†’ image-annotator-lib
```

**Key Components**
- **WorkerService** (`src/lorairo/gui/services/worker_service.py`): Qt-based worker coordination
- **AnnotationWorker** (`src/lorairo/gui/workers/annotation_worker.py`): QRunnable-based asynchronous processing
- **AnnotationService** (`src/lorairo/services/annotation_service.py`): Legacy business logic (deprecated)
- **ai_annotator.py** (`src/lorairo/annotations/ai_annotator.py`): Library integration wrapper
  - `get_available_annotator_models()`: Retrieve available AI models
  - `call_annotate_library()`: Execute annotation with comprehensive error handling
  - `AiAnnotatorError`: Custom exception for library-specific errors
- **image-annotator-lib**: External library providing unified AI provider access

**Data Flow**
- Input: `list[PIL.Image]` + `list[str]` (model names) + `list[str]` (pHash values)
- Processing: Multi-provider AI annotation via unified `annotate()` function
- Output: `PHashAnnotationResults` with structured annotation data
- Storage: Results processed by AnnotationService and stored via DatabaseManager

**Current Implementation Status**
- âœ… **Fully Integrated**: image-annotator-lib for AI annotation
- âœ… **Fully Integrated**: genai-tag-db-tools for tag cleaning and database operations
- âœ… **Active**: Clean separation between legacy and current implementation
- ğŸ”„ **In Progress**: Legacy code cleanup and documentation alignment

### Development Tools

#### Code Quality
- **Ruff**: Fast Python linter and formatter
  - Replaces multiple tools (flake8, black, isort)
  - Rust-based for performance
  - Configurable rules and formatting
- **mypy**: Static type checking
  - Type hint validation
  - Runtime error prevention
  - IDE integration support

#### Testing
- **pytest**: Testing framework
  - Fixture-based test organization
  - Parametrized testing
  - Plugin ecosystem
- **pytest-cov**: Coverage analysis
  - Line and branch coverage
  - HTML and XML reports
  - Minimum coverage enforcement
- **pytest-qt**: GUI testing
  - Qt application testing
  - Event simulation
  - Widget interaction testing

#### Logging
- **Loguru**: Modern structured logging (migrated from standard logging)
  - Simple, intuitive API
  - Colored output for development
  - File rotation and retention
  - Context management and filtering
  - Better performance than standard logging
  - Configured via `src/lorairo/utils/log.py`

## Development Environment

### System Requirements

#### Operating System
- **Windows 10/11**: Primary development platform
- **macOS 10.15+**: Supported with Qt compatibility
- **Linux**: Ubuntu 20.04+ or equivalent

#### Hardware Requirements
- **Memory**: 8GB RAM minimum, 16GB recommended
- **Storage**: 10GB available space for models and data
- **GPU**: Optional but recommended for local ML models
  - NVIDIA GPU with CUDA support
  - 4GB+ VRAM for larger models

#### Python Environment
- **Python 3.11 or higher**
- **Virtual environment support**
- **Package compilation tools** (for some dependencies)

### Environment Setup

#### Initial Setup
```bash
# Clone repository with submodules
git clone --recurse-submodules https://github.com/user/LoRAIro.git
cd LoRAIro

# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create and activate virtual environment with dependencies
uv sync --dev

# Verify installation
uv run python -c "import lorairo; print('Installation successful')"
```

#### Development Dependencies
```toml
[tool.uv.dev-dependencies]
pytest = "^8.0.0"
pytest-cov = "^4.0.0"
pytest-qt = "^4.2.0"
ruff = "^0.1.0"
mypy = "^1.7.0"
pre-commit = "^3.5.0"
```

#### Local Package Integration
```toml
[tool.uv.sources]
genai-tag-db-tools = { path = "local_packages/genai-tag-db-tools", editable = true }
image-annotator-lib = { path = "local_packages/image-annotator-lib", editable = true }
```

### Configuration Management

#### Configuration Files
- **`config/lorairo.toml`**: Main application configuration
  - Database settings
  - Image processing parameters
  - Logging configuration
  - UI preferences
- **`config/annotator_config.toml`**: AI annotator configuration
  - Model definitions and parameters
  - Provider-specific settings
- **`config/available_api_models.toml`**: API model definitions
  - Supported AI provider models
  - Model capabilities and limits

#### Configuration Service
- **`src/lorairo/services/configuration_service.py`**: Configuration management
  - TOML file parsing and validation
  - Runtime configuration access
  - Settings persistence
- **`src/lorairo/utils/config.py`**: Configuration utilities
  - Configuration loading helpers
  - Default value management

### Configuration Management

#### Primary Configuration (`config/lorairo.toml`)
```toml
[app]
name = "LoRAIro"
version = "1.0.0"
debug = false

[database]
path = "Image_database/image_database.db"
pool_size = 5
echo = false

[ai_providers.openai]
api_key_env = "OPENAI_API_KEY"
model = "gpt-4-vision-preview"
timeout = 30
retry_attempts = 3
rate_limit_requests_per_minute = 10

[ai_providers.anthropic]
api_key_env = "ANTHROPIC_API_KEY"
model = "claude-3-sonnet-20240229"
timeout = 45
retry_attempts = 2

[ai_providers.google]
api_key_env = "GOOGLE_API_KEY"
model = "gemini-pro-vision"
timeout = 30
retry_attempts = 3

[logging]
level = "INFO"
format = "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
rotation = "1 week"
retention = "1 month"
```

#### Environment Variables
```bash
# AI Provider API Keys
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
export GOOGLE_API_KEY="your-google-api-key"

# Development Settings
export LORAIRO_DEBUG="true"
export LORAIRO_LOG_LEVEL="DEBUG"
```

## Database Design

### Schema Architecture

#### Core Tables

**Images Table**
```sql
CREATE TABLE images (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    file_path TEXT NOT NULL UNIQUE,
    file_name TEXT NOT NULL,
    file_size INTEGER,
    width INTEGER,
    height INTEGER,
    format TEXT,
    hash_value TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Annotations Table**
```sql
CREATE TABLE annotations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    image_id INTEGER NOT NULL,
    provider TEXT NOT NULL,
    model TEXT NOT NULL,
    caption TEXT,
    tags TEXT, -- JSON array
    confidence REAL,
    processing_time REAL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (image_id) REFERENCES images (id) ON DELETE CASCADE
);
```

**Quality Scores Table**
```sql
CREATE TABLE quality_scores (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    image_id INTEGER NOT NULL,
    aesthetic_score REAL,
    technical_score REAL,
    overall_score REAL,
    scoring_model TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (image_id) REFERENCES images (id) ON DELETE CASCADE
);
```

#### Indexes and Performance
```sql
-- Performance indexes
CREATE INDEX idx_images_file_path ON images(file_path);
CREATE INDEX idx_images_hash ON images(hash_value);
CREATE INDEX idx_annotations_image_id ON annotations(image_id);
CREATE INDEX idx_annotations_provider ON annotations(provider);
CREATE INDEX idx_quality_scores_image_id ON quality_scores(image_id);
```

#### Timezone Handling (2025/07/12 Standardization)

**UTC Standardization Policy**
- **Database Storage**: All `TIMESTAMP(timezone=True)` fields store timezone-aware UTC datetime objects
- **Application Processing**: All datetime parsing and comparison operations use UTC timezone consistency
- **Date String Processing**: The `_parse_datetime_str()` method in `ImageRepository` ensures:
  - Naive datetime strings are interpreted as UTC and converted to timezone-aware objects
  - Existing timezone-aware datetime objects preserve their timezone information
  - UTC timezone-aware objects are returned for database comparison compatibility
  - Invalid date formats return `None` with appropriate warning logging

**Implementation Details**
```python
# Database schema uses timezone-aware TIMESTAMP fields
created_at = Column(TIMESTAMP(timezone=True), default=lambda: datetime.now(UTC))
updated_at = Column(TIMESTAMP(timezone=True), default=lambda: datetime.now(UTC))

# Application datetime parsing ensures UTC consistency
def _parse_datetime_str(self, date_str: str | None) -> datetime | None:
    # Parses date strings and returns timezone-aware UTC datetime objects
    # Ensures compatibility with TIMESTAMP(timezone=True) database fields
```

**Benefits**
- Consistent timezone handling across all database operations
- Proper comparison operations between parsed dates and database timestamps
- Protection against timezone-related bugs in filtering and search operations
- Standardized UTC storage for international deployment compatibility

### Migration Management

#### Alembic Configuration
```python
# alembic/env.py
from sqlalchemy import engine_from_config, pool
from alembic import context
from lorairo.database.schema import Base

target_metadata = Base.metadata

def run_migrations_online():
    configuration = context.config
    configuration.set_main_option(
        "sqlalchemy.url", 
        get_database_url()
    )
    
    connectable = engine_from_config(
        configuration.get_section(configuration.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()
```

#### Migration Best Practices
- Always review auto-generated migrations
- Test migrations with sample data
- Provide rollback procedures
- Document breaking changes
- Backup database before production migrations

## Code Architecture

### Design Patterns

#### Repository Pattern
```python
from abc import ABC, abstractmethod
from typing import Optional, List
from lorairo.database.schema import ImageRecord

class ImageRepositoryInterface(ABC):
    @abstractmethod
    def create(self, image_data: dict) -> ImageRecord:
        pass
    
    @abstractmethod
    def get_by_id(self, image_id: int) -> Optional[ImageRecord]:
        pass
    
    @abstractmethod
    def get_by_path(self, file_path: str) -> Optional[ImageRecord]:
        pass
    
    @abstractmethod
    def list_all(self, limit: int = 100, offset: int = 0) -> List[ImageRecord]:
        pass

class SQLAlchemyImageRepository(ImageRepositoryInterface):
    def __init__(self, session_factory):
        self.session_factory = session_factory
    
    def create(self, image_data: dict) -> ImageRecord:
        with self.session_factory() as session:
            image = ImageRecord(**image_data)
            session.add(image)
            session.commit()
            session.refresh(image)
            return image
```

#### Service Layer Pattern
```python
from typing import List, Dict, Any
from pathlib import Path
from lorairo.database.db_repository import ImageRepository
from lorairo.storage.file_system import FileSystemManager

class ImageProcessingService:
    def __init__(
        self,
        image_repository: ImageRepository,
        file_manager: FileSystemManager,
        config: dict
    ):
        self.image_repository = image_repository
        self.file_manager = file_manager
        self.config = config
        self.logger = get_logger(__name__)
    
    async def process_images(
        self, 
        image_paths: List[Path]
    ) -> List[Dict[str, Any]]:
        """Process multiple images with error handling and progress tracking."""
        results = []
        
        for i, path in enumerate(image_paths):
            try:
                result = await self._process_single_image(path)
                results.append(result)
                
                # Emit progress signal
                progress = int((i + 1) / len(image_paths) * 100)
                self.progress_updated.emit(progress)
                
            except Exception as e:
                self.logger.error(f"Failed to process {path}: {e}")
                results.append({"error": str(e), "path": str(path)})
        
        return results
```

#### Factory Pattern
```python
from typing import Protocol
from lorairo.annotations.providers.base import AIProvider

class AIProviderFactory:
    _providers = {
        "openai": "lorairo.annotations.providers.openai.OpenAIProvider",
        "anthropic": "lorairo.annotations.providers.anthropic.AnthropicProvider",
        "google": "lorairo.annotations.providers.google.GoogleProvider",
    }
    
    @classmethod
    def create_provider(cls, provider_name: str, config: dict) -> AIProvider:
        if provider_name not in cls._providers:
            raise ValueError(f"Unknown provider: {provider_name}")
        
        provider_class_path = cls._providers[provider_name]
        module_path, class_name = provider_class_path.rsplit(".", 1)
        
        module = importlib.import_module(module_path)
        provider_class = getattr(module, class_name)
        
        return provider_class(config)
```

### Error Handling Strategy

#### Custom Exception Hierarchy
```python
class LoRAIroException(Exception):
    """Base exception for LoRAIro application."""
    pass

class ConfigurationError(LoRAIroException):
    """Configuration-related errors."""
    pass

class DatabaseError(LoRAIroException):
    """Database operation errors."""
    pass

class ImageProcessingError(LoRAIroException):
    """Image processing errors."""
    pass

class AIProviderError(LoRAIroException):
    """AI provider communication errors."""
    pass

class RateLimitError(AIProviderError):
    """API rate limit exceeded."""
    pass

class AuthenticationError(AIProviderError):
    """API authentication failed."""
    pass
```

#### Error Handling Pattern
```python
from contextlib import contextmanager
from typing import Generator

@contextmanager
def handle_ai_provider_errors() -> Generator[None, None, None]:
    """Context manager for consistent AI provider error handling."""
    try:
        yield
    except requests.exceptions.Timeout:
        raise AIProviderError("Request timeout - provider may be overloaded")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429:
            raise RateLimitError("Rate limit exceeded")
        elif e.response.status_code == 401:
            raise AuthenticationError("Invalid API credentials")
        else:
            raise AIProviderError(f"HTTP error: {e}")
    except Exception as e:
        raise AIProviderError(f"Unexpected error: {e}")
```

### Type System

#### Type Definitions
```python
from typing import TypedDict, Optional, List, Union, Literal
from pathlib import Path

class ImageMetadata(TypedDict):
    width: int
    height: int
    format: str
    file_size: int
    hash_value: str

class AnnotationResult(TypedDict):
    caption: Optional[str]
    tags: List[str]
    confidence: float
    processing_time: float
    provider: str
    model: str

class QualityScore(TypedDict):
    aesthetic_score: float
    technical_score: float
    overall_score: float
    scoring_model: str

AIProvider = Literal["openai", "anthropic", "google"]
ImageFormat = Literal["JPEG", "PNG", "WebP", "BMP", "TIFF"]
ProcessingStatus = Literal["pending", "processing", "completed", "failed"]
```

#### Protocol Definitions
```python
from typing import Protocol, runtime_checkable

@runtime_checkable
class ImageProcessor(Protocol):
    def process_image(self, image_path: Path) -> ImageMetadata:
        """Process an image and return metadata."""
        ...

@runtime_checkable
class AnnotationProvider(Protocol):
    async def annotate_image(self, image_path: Path) -> AnnotationResult:
        """Generate annotations for an image."""
        ...

@runtime_checkable
class QualityScorer(Protocol):
    def score_image(self, image_path: Path) -> QualityScore:
        """Calculate quality scores for an image."""
        ...
```

## Performance Optimization

### Memory Management

#### Image Processing Optimization
```python
from contextlib import contextmanager
import psutil
from PIL import Image

@contextmanager
def memory_limited_processing(max_memory_mb: int = 1000):
    """Context manager to limit memory usage during processing."""
    initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
    
    try:
        yield
    finally:
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_used = current_memory - initial_memory
        
        if memory_used > max_memory_mb:
            import gc
            gc.collect()
            logger.warning(f"High memory usage detected: {memory_used:.1f}MB")

def process_large_image(image_path: Path) -> Image.Image:
    """Process large images with memory optimization."""
    with memory_limited_processing():
        # Open image with lazy loading
        with Image.open(image_path) as img:
            # Process in chunks if needed
            if img.size[0] * img.size[1] > 4000 * 4000:
                # Resize for processing
                img.thumbnail((2048, 2048), Image.Resampling.LANCZOS)
            
            # Convert to RGB if needed
            if img.mode != "RGB":
                img = img.convert("RGB")
            
            return img.copy()
```

#### Database Connection Pooling
```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool

class DatabaseManager:
    def __init__(self, database_url: str):
        self.engine = create_engine(
            database_url,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    @contextmanager
    def get_session(self):
        session = self.SessionLocal()
        try:
            yield session
            session.commit()
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()
```

### Asynchronous Processing

#### Hybrid Controlled Batch Processing (clarified 2025/07/06)
```python
class HybridBatchProcessor:
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.performance_target = 300  # 5 minutes for 1000 images = 300 seconds
        
    async def process_images_batch(
        self,
        image_paths: List[Path],
        progress_callback: Callable[[int], None] | None = None
    ) -> List[ProcessingResult]:
        """Process images in 100-image batches for optimal memory control."""
        total_images = len(image_paths)
        processed_count = 0
        
        for batch_start in range(0, total_images, self.batch_size):
            batch_end = min(batch_start + self.batch_size, total_images)
            batch = image_paths[batch_start:batch_end]
            
            # Batch registration and processing
            batch_results = await self._process_single_batch(batch)
            processed_count += len(batch_results)
            
            if progress_callback:
                progress = int(processed_count / total_images * 100)
                progress_callback(progress)
                
        return results

#### Policy Violation Tracking (clarified 2025/07/06)
```python
# Database schema addition for policy violation tracking
class AnnotationFailure(Base):
    __tablename__ = 'annotation_failures'
    
    id = Column(Integer, primary_key=True)
    image_id = Column(Integer, ForeignKey('images.id'))
    model_name = Column(String, nullable=False)
    provider = Column(String, nullable=False)
    failure_reason = Column(String, nullable=False)
    error_message = Column(Text)
    failed_at = Column(DateTime, default=datetime.utcnow)
    is_policy_violation = Column(Boolean, default=False)

# Retry policy with violation awareness
def check_policy_violations(image_ids: List[int], model_name: str) -> List[int]:
    """Check for previous policy violations and warn user."""
    violations = session.query(AnnotationFailure).filter(
        AnnotationFailure.image_id.in_(image_ids),
        AnnotationFailure.model_name == model_name,
        AnnotationFailure.is_policy_violation == True
    ).all()
    
    if violations:
        # Show warning dialog to user
        show_policy_violation_warning(violations)
    
    return [v.image_id for v in violations]
```

#### Background Task Management
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Callable, Any

class TaskManager:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.active_tasks: List[asyncio.Task] = []
    
    async def run_in_background(
        self, 
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """Run CPU-bound task in background thread."""
        loop = asyncio.get_event_loop()
        task = loop.run_in_executor(self.executor, func, *args, **kwargs)
        self.active_tasks.append(task)
        
        try:
            result = await task
            return result
        finally:
            self.active_tasks.remove(task)
    
    async def process_batch(
        self,
        items: List[Any],
        processor: Callable,
        batch_size: int = 10
    ) -> List[Any]:
        """Process items in batches to control resource usage."""
        results = []
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_tasks = [
                self.run_in_background(processor, item)
                for item in batch
            ]
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            results.extend(batch_results)
        
        return results
```

## Security Considerations

### API Key Management

#### ConfigurationService Requirements (updated 2025/07/07)

**Configuration Item Structure**
```toml
[api]
openai_key = ""          # OpenAI APIã‚­ãƒ¼ (å¹³æ–‡ä¿å­˜)
claude_key = ""          # Anthropic Claude APIã‚­ãƒ¼ (å¹³æ–‡ä¿å­˜)
google_key = ""          # Google Vision APIã‚­ãƒ¼ (å¹³æ–‡ä¿å­˜)

[directories]
database_dir = ""        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç¾¤ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆproject_name/database.db + images/ï¼‰
export_dir = ""          # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®å‡ºåŠ›å…ˆï¼ˆ.txt/.captionãƒ•ã‚¡ã‚¤ãƒ«ç­‰ï¼‰
batch_results_dir = ""   # OpenAI Batch APIçµæœJSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆ

[huggingface]
hf_username = ""         # Hugging Face ãƒ¦ãƒ¼ã‚¶ãƒ¼å (å¹³æ–‡ä¿å­˜)
repo_name = ""           # ãƒªãƒã‚¸ãƒˆãƒªå
token = ""               # Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ (å¹³æ–‡ä¿å­˜)

[log]
level = "INFO"           # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (DEBUG/INFO/WARNING/ERROR/CRITICAL)
file_path = ""           # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
```

**Directory Structure Design**
```
database_dir/
â”œâ”€â”€ project_a/
â”‚   â”œâ”€â”€ database.db      # SQLite database for project_a
â”‚   â””â”€â”€ images/          # Processed images for project_a
â””â”€â”€ project_b/
    â”œâ”€â”€ database.db      # SQLite database for project_b  
    â””â”€â”€ images/          # Processed images for project_b
```

**Functional Requirements (2025/07/06 clarifications)**
- **API Key Management**: Plain-text storage in config.toml (personal OSS development)
- **Logging Security**: API keys masked with `***` in log output
- **UI Integration**: Auto-exclude models from providers with missing API keys
- **Configuration Changes**: Immediate reflection and file persistence
- **Validation**: Path format validation, log level validation, required field checks

**Configuration Item Changes**
- `database` â†’ `database_dir` (multi-DB file support)
- `response_file` â†’ `batch_results_dir` (OpenAI Batch API results)
- `output` â†’ `export_dir` (annotation results export)
- `edited_output` â†’ removed (not used)
- `dataset` â†’ managed under `database_dir` structure

**Architecture Design (implemented 2025/07/07)**

**Shared Configuration with Dependency Injection Pattern**
- Multiple ConfigurationService instances share the same configuration object (reference passing)
- Configuration changes are immediately reflected across all instances
- High testability through dependency injection of mock configurations
- Automatic default configuration file creation when missing

**Usage Pattern**
```python
# Master configuration service (loads file)
master_config = ConfigurationService()
shared_config = master_config.get_shared_config()

# Child services (share configuration object)
window_config = ConfigurationService(shared_config=shared_config)
processor_config = ConfigurationService(shared_config=shared_config)

# Changes propagate immediately to all instances
window_config.update_setting("api", "openai_key", "new_key")
# â†’ processor_config also sees the change instantly
```

**Project Structure Design (updated 2025/07/07)**

**Multi-Project Database Architecture**
- Each project maintains independent SQLite database for data isolation
- Unified main database for cross-project search and analysis
- Support for project extraction workflows (subset creation)

**Directory Structure Pattern**
```
lorairo_data/
â”œâ”€â”€ {project_name}_{YYYYMMDD}_{NNN}/
â”‚   â”œâ”€â”€ image_database.db
â”‚   â””â”€â”€ image_dataset/
â”‚       â”œâ”€â”€ original_images/{YYYY}/{MM}/{DD}/{source_dir}/
â”‚       â”œâ”€â”€ {resolution}/{YYYY}/{MM}/{DD}/{source_dir}/
â”‚       â””â”€â”€ batch_request_jsonl/
```

**Project Name Support**
- Unicode project names supported (Japanese, mixed languages)
- Safe filename sanitization for filesystem compatibility
- Date-based versioning with 3-digit incremental numbering

**Use Cases**
- Main dataset management with comprehensive search
- Focused project extraction (quality filters, content type, tags)
- HuggingFace dataset preparation and publishing
- Research dataset curation with provenance tracking

**Implementation Requirements**
```python
def get_project_dir(base_dir_name: str, project_name: str = "project") -> Path:
    """Generate project directory with Unicode name support"""

def normalize_legacy_paths(db_path: Path) -> None:
    """Convert legacy absolute paths to relative project paths"""

def __init__(self, config_path: Path | None = None, shared_config: dict[str, Any] | None = None):
    """Initialize with optional shared configuration object for DI pattern"""
    
def _create_default_config_file(self) -> dict[str, Any]:
    """Create default configuration file when missing"""
    
def mask_api_key(key: str) -> str:
    """APIã‚­ãƒ¼ã‚’***ã§ãƒã‚¹ã‚­ãƒ³ã‚°"""
    if not key or len(key) < 8:
        return "***"
    return f"{key[:4]}***{key[-4:]}"

def get_available_providers(self) -> list[str]:
    """APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿”ã™"""
    providers = []
    if self.get_setting("api", "openai_key"):
        providers.append("openai")
    if self.get_setting("api", "claude_key"):
        providers.append("anthropic")
    if self.get_setting("api", "google_key"):
        providers.append("google")
    return providers
```

### Input Validation

#### File Path Validation
```python
from pathlib import Path
import mimetypes

class FileValidator:
    ALLOWED_EXTENSIONS = {".jpg", ".jpeg", ".png", ".webp", ".bmp", ".tiff"}
    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    
    @classmethod
    def validate_image_file(cls, file_path: Path) -> bool:
        """Validate image file for security and format compliance."""
        
        # Check if file exists
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Check file extension
        if file_path.suffix.lower() not in cls.ALLOWED_EXTENSIONS:
            raise ValueError(f"Unsupported file type: {file_path.suffix}")
        
        # Check file size
        if file_path.stat().st_size > cls.MAX_FILE_SIZE:
            raise ValueError(f"File too large: {file_path.stat().st_size} bytes")
        
        # Check MIME type
        mime_type, _ = mimetypes.guess_type(str(file_path))
        if not mime_type or not mime_type.startswith("image/"):
            raise ValueError(f"Invalid MIME type: {mime_type}")
        
        # Validate image content
        try:
            with Image.open(file_path) as img:
                img.verify()
        except Exception as e:
            raise ValueError(f"Invalid image file: {e}")
        
        return True
```

## Deployment Considerations

### Build and Distribution

#### Application Packaging
```python
# pyproject.toml
[project]
name = "lorairo"
version = "1.0.0"
description = "AI-powered image annotation for ML datasets"
authors = [{name = "Your Name", email = "your.email@example.com"}]
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.11"

dependencies = [
    "PySide6>=6.6.0",
    "SQLAlchemy>=2.0.0",
    "alembic>=1.12.0",
    "Pillow>=10.0.0",
    "requests>=2.31.0",
    "loguru>=0.7.0",
    "toml>=0.10.0",
    "psutil>=5.9.0",
]

[project.scripts]
lorairo = "lorairo.main:main"

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-qt>=4.2.0",
    "ruff>=0.1.0",
    "mypy>=1.7.0",
    "pre-commit>=3.5.0",
]
```

#### Environment Configuration
```bash
# Production environment setup
export LORAIRO_ENV="production"
export LORAIRO_LOG_LEVEL="INFO"
export LORAIRO_DATABASE_PATH="/opt/lorairo/data/database.db"
export LORAIRO_CONFIG_PATH="/etc/lorairo/config.toml"

# Resource limits
export LORAIRO_MAX_MEMORY_MB="2048"
export LORAIRO_MAX_WORKERS="4"
export LORAIRO_BATCH_SIZE="10"
```

## Image Processing Module Architecture

### Module Separation Design (2025/07/14 - Completed)

#### Overview
The image processing system has been successfully refactored from a monolithic approach into modular components following clean architecture principles and dependency injection patterns.

#### Completed Module Structure

**Current Implementation (`src/lorairo/editor/`)**
- **`image_processor.py`**: ImageProcessingManager and ImageProcessor classes
  - High-level processing coordination and workflow management
  - Image resizing and color profile normalization
  - Integration point for AutoCrop and Upscaler modules
- **`autocrop.py`**: AutoCrop module (Completed 2025/07/12)
  - Singleton pattern with classmethod interface
  - Complementary color difference algorithm
  - Letterbox detection and removal
- **`upscaler.py`**: Upscaler module (Completed 2025/07/14)
  - Configuration-driven dependency injection
  - Multi-model support (RealESRGAN variants)
  - Model caching and lazy loading

#### Implementation Details

**1. ImageProcessingManager Integration**
```python
class ImageProcessingManager:
    def __init__(self, file_system_manager: FileSystemManager, target_resolution: int, 
                 preferred_resolutions: list[tuple[int, int]], config_service: ConfigurationService):
        self.upscaler = Upscaler(config_service)  # Dependency injection
        
    def process_image(self, db_stored_original_path: Path, original_has_alpha: bool, 
                     original_mode: str, upscaler: str | None = None) -> tuple[Image.Image | None, dict[str, Any]]:
        cropped_img = AutoCrop.auto_crop_image(img)  # Static method call
        upscaled_img = self.upscaler.upscale_image(converted_img, upscaler)  # Instance method
```

**2. AutoCrop Module (Singleton Pattern)**
```python
class AutoCrop:
    """Singleton pattern with complementary color difference algorithm"""
    
    @classmethod
    def auto_crop_image(cls, pil_image: Image.Image) -> Image.Image:
        """Main public interface for automatic image cropping"""
        
    def _get_crop_area(self, np_image: np.ndarray) -> tuple[int, int, int, int] | None:
        """Core complementary color analysis algorithm"""
```

**3. Upscaler Module (Dependency Injection Pattern)**
```python
class Upscaler:
    """Configuration-driven upscaler with dependency injection"""
    
    def __init__(self, config_service: ConfigurationService):
        self.config_service = config_service
        self._loaded_models: dict[str, Any] = {}  # Model caching
        
    @classmethod
    def create_default(cls) -> "Upscaler":
        """Factory method for backward compatibility"""
        
    def upscale_image(self, img: Image.Image, model_name: str, scale: float | None = None) -> Image.Image:
        """Main upscaling interface with model management"""
        """æŒ‡å®šè§£åƒåº¦ã¸ã®ç”»åƒã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«å‡¦ç†"""
        
    def get_available_models(self) -> list[str]:
        """åˆ©ç”¨å¯èƒ½ãªã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§å–å¾—"""
```

#### Database Schema Enhancements

**ProcessedImage Table Extension**
```python
class ProcessedImage(Base):
    # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    upscaler_used: Mapped[str | None] = mapped_column(String)  # ä½¿ç”¨ã•ã‚ŒãŸã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å
    
    # æ–°è¦è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (2025/07/12)
    crop_status: Mapped[str | None] = mapped_column(String)    # ã‚¯ãƒ­ãƒƒãƒ—çŠ¶æ…‹: None/"auto"/"approved"/"manual"
```

**Crop Status Management**
- **None**: æœªå‡¦ç† (æ–°è¦ç”»åƒã€ã‚¯ãƒ­ãƒƒãƒ—å‡¦ç†å®Ÿè¡Œ)
- **"auto"**: è‡ªå‹•ã‚¯ãƒ­ãƒƒãƒ—æ¸ˆã¿ (å†å‡¦ç†å¯¾è±¡)
- **"approved"**: æ‰¿èªæ¸ˆã¿ã‚¯ãƒ­ãƒƒãƒ— (å†å‡¦ç†å¯¾è±¡å¤–)
- **"manual"**: æ‰‹å‹•èª¿æ•´æ¸ˆã¿ã‚¯ãƒ­ãƒƒãƒ— (å†å‡¦ç†å¯¾è±¡å¤–)

#### Implementation Specifications

**1. Image ID Management**
- **åŸºæœ¬åŸå‰‡**: å…¨ã¦ã®å‡¦ç†æ¸ˆã¿ç”»åƒã¯å…ƒç”»åƒã®image_idã‚’åŸºæº–ã¨ã™ã‚‹
- **ä¸€æ„æ€§ä¿è¨¼**: `(image_id, width, height, filename)` ã«ã‚ˆã‚‹è¤‡åˆãƒ¦ãƒ‹ãƒ¼ã‚¯åˆ¶ç´„
- **é–¢ä¿‚æ€§ç¶­æŒ**: å…ƒç”»åƒå‰Šé™¤æ™‚ã®ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ (`ondelete="CASCADE"`)

**2. Crop Processing Pipeline**
```python
def process_with_crop_awareness(self, image_id: int, image_path: Path) -> ProcessingResult:
    """ã‚¯ãƒ­ãƒƒãƒ—çŠ¶æ…‹ã‚’è€ƒæ…®ã—ãŸå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    # 1. æ—¢å­˜å‡¦ç†æ¸ˆã¿ç”»åƒã®ã‚¯ãƒ­ãƒƒãƒ—çŠ¶æ…‹ç¢ºèª
    existing_metadata = self.db_manager.get_processed_metadata(image_id)
    crop_status = existing_metadata.get('crop_status') if existing_metadata else None
    
    # 2. æ‰¿èªæ¸ˆã¿ç”»åƒã¯å†å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—
    if crop_status in ["approved", "manual"]:
        logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: æ‰¿èªæ¸ˆã¿ã‚¯ãƒ­ãƒƒãƒ— (status={crop_status})")
        return existing_metadata
        
    # 3. è‡ªå‹•ã‚¯ãƒ­ãƒƒãƒ—å®Ÿè¡Œ (None, "auto"ã®å ´åˆ)
    processed_image, processing_metadata = self.auto_crop.crop_image(image, crop_status)
    processing_metadata['crop_status'] = 'auto'  # å‡¦ç†å¾Œã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¨­å®š
```

**3. Upscaler Model Management**

**Fixed Directory Approach (Simple)**
```python
class Upscaler:
    MODEL_DIRECTORY = Path("models/upscalers")
    
    def get_available_models(self) -> list[str]:
        """models/upscalerså†…ã®å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—"""
        if not self.MODEL_DIRECTORY.exists():
            return []
            
        model_files = []
        for file_path in self.MODEL_DIRECTORY.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in ['.pth', '.safetensors', '.pt', '.ckpt']:
                model_files.append(file_path.stem)
        
        return sorted(model_files)
```

**Spandrel Integration Specification**
```python
from spandrel import ModelLoader, UnsupportedModelError

class SpandrelUpscaler:
    def __init__(self):
        self.model_loader = ModelLoader(device="cuda" if torch.cuda.is_available() else "cpu")
        
    def load_model(self, model_path: Path) -> bool:
        """Spandrelã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¨äº’æ›æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            model_descriptor = self.model_loader.load_from_file(model_path)
            logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {model_path.name}")
            return True
        except UnsupportedModelError:
            logger.warning(f"Spandreléå¯¾å¿œãƒ¢ãƒ‡ãƒ«: {model_path.name}")
            return False
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {model_path.name}, Error: {e}")
            return False
```

**Model Directory Structure**
```
models/
â””â”€â”€ upscalers/
    â”œâ”€â”€ RealESRGAN_x4plus.pth
    â”œâ”€â”€ waifu2x_art_noise3_scale2.safetensors
    â”œâ”€â”€ ESRGAN_x4.pth
    â””â”€â”€ model_symlink.pth -> /path/to/actual/model.pth
```

#### Performance and Resource Management

**Memory Optimization**
```python
class ResourceAwareProcessor:
    def __init__(self, max_memory_mb: int = 2048):
        self.max_memory_mb = max_memory_mb
        
    def process_with_memory_limit(self, image: Image.Image) -> Image.Image:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ãªãŒã‚‰å‡¦ç†å®Ÿè¡Œ"""
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        try:
            result = self._execute_processing(image)
            return result
        finally:
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024
            if current_memory - initial_memory > self.max_memory_mb * 0.8:
                gc.collect()
                logger.warning(f"é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¤œå‡º: {current_memory - initial_memory:.1f}MB")
```

**Batch Processing Integration**
```python
def process_batch_with_crop_status(self, image_ids: list[int]) -> list[ProcessingResult]:
    """ãƒãƒƒãƒå‡¦ç†ã§ã®ã‚¯ãƒ­ãƒƒãƒ—çŠ¶æ…‹è€ƒæ…®"""
    results = []
    
    for image_id in image_ids:
        # æ‰¿èªæ¸ˆã¿ç”»åƒã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if self._is_approved_crop(image_id):
            logger.debug(f"ã‚¹ã‚­ãƒƒãƒ—: æ‰¿èªæ¸ˆã¿ã‚¯ãƒ­ãƒƒãƒ— (ID={image_id})")
            continue
            
        # æœªå‡¦ç†ãƒ»è‡ªå‹•å‡¦ç†æ¸ˆã¿ç”»åƒã®å‡¦ç†å®Ÿè¡Œ
        result = self.process_with_crop_awareness(image_id)
        results.append(result)
        
    return results
```

#### Migration and Implementation Strategy

**Phase 1: Module Extraction**
1. `auto_crop.py` - AutoCropã‚¯ãƒ©ã‚¹ã®åˆ†é›¢
2. `upscaler.py` - Upscalerã‚¯ãƒ©ã‚¹ã®åˆ†é›¢
3. `processing_manager.py` - ImageProcessingManagerã®æ›´æ–°

**Phase 2: Database Schema Migration**
```python
"""Add crop_status to processed_images table

Revision ID: add_crop_status
Revises: previous_revision
Create Date: 2025-07-12
"""

def upgrade():
    op.add_column('processed_images', 
                  sa.Column('crop_status', sa.String(), nullable=True))

def downgrade():
    op.drop_column('processed_images', 'crop_status')
```

**Phase 3: Integration Testing**
- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æ¤œè¨¼
- ã‚¯ãƒ­ãƒƒãƒ—çŠ¶æ…‹ç®¡ç†ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼  
- Spandrelãƒ¢ãƒ‡ãƒ«äº’æ›æ€§ãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

#### Benefits and Design Rationale

**Maintainability Improvements**
- **Single Responsibility**: å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒæ˜ç¢ºãªè²¬å‹™ã‚’æŒã¤
- **Testability**: ç‹¬ç«‹ã—ãŸãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãŒå¯èƒ½
- **Reusability**: ä»–ã®å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®å†åˆ©ç”¨æ€§å‘ä¸Š

**Performance Optimizations**
- **Memory Management**: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å˜ä½ã§ã®æœ€é©åŒ–
- **Resource Isolation**: ã‚¯ãƒ­ãƒƒãƒ—ã¨ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ã®ç‹¬ç«‹å®Ÿè¡Œ
- **Batch Efficiency**: æ‰¿èªæ¸ˆã¿ç”»åƒã®ã‚¹ã‚­ãƒƒãƒ—ã«ã‚ˆã‚‹å‡¦ç†æ™‚é–“çŸ­ç¸®

**Extensibility**
- **Model Support**: æ–°ã—ã„ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç°¡å˜ãªè¿½åŠ 
- **Processing Options**: ã‚¯ãƒ­ãƒƒãƒ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å·®ã—æ›¿ãˆå¯èƒ½
- **Workflow Customization**: å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æŸ”è»Ÿãªèª¿æ•´

## Upscaler Resource Management Implementation Plan (2025/07/12)

### Overview
Spandrelãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç®¡ç†ã®å®Ÿè£…è¨ˆç”»ã€‚ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡åŒ–ã¨VRAMãƒªã‚½ãƒ¼ã‚¹ç«¶åˆå›é¿ã‚’ç›®çš„ã¨ã™ã‚‹ã€‚

### Resource Management Strategy

#### Instance Lifecycle (Confirmed Specification)
```
1. åˆå›ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«å®Ÿè¡Œ â†’ ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ + ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
2. ãƒãƒƒãƒå‡¦ç†å®Œäº† â†’ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç¶­æŒï¼ˆåŒãƒ¢ãƒ‡ãƒ«ä¿æŒï¼‰
3. åŒãƒ¢ãƒ‡ãƒ«ã§ã®åˆ¥ãƒãƒƒãƒ â†’ æ—¢å­˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å†åˆ©ç”¨ï¼ˆèª­ã¿è¾¼ã¿ä¸è¦ï¼‰
4. ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«æŒ‡å®š â†’ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç ´æ£„ â†’ æ–°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ + æ–°ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
5. ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†é–‹å§‹ â†’ å¼·åˆ¶ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç ´æ£„ + VRAMè§£æ”¾
6. ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã®ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ« â†’ æ–°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆï¼ˆå‰å›ã¨åŒãƒ¢ãƒ‡ãƒ«ã§ã‚‚ï¼‰
```

#### Design Principles
- **Single Instance Pattern**: ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«1ã¤ã®Upscalerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã¿ä¿æŒ
- **Model Change Detection**: ãƒ¢ãƒ‡ãƒ«åå¤‰æ›´æ™‚ã®ã¿æ–°è¦èª­ã¿è¾¼ã¿å®Ÿè¡Œ
- **Forced Cleanup**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†å‰ã®å¼·åˆ¶ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
- **No Caching**: ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å®Ÿè£…ã—ãªã„ï¼ˆã‚³ã‚¹ãƒˆãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆä¸é©åˆï¼‰

### Implementation Specification

#### Core Class Structure
```python
class Upscaler:
    """Spandrelãƒ™ãƒ¼ã‚¹ã®ç”»åƒã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    
    # ã‚¯ãƒ©ã‚¹å¤‰æ•°ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ï¼‰
    _global_instance: "Upscaler | None" = None
    _current_model_name: str | None = None
    
    @classmethod
    def get_for_model(cls, model_name: str) -> "Upscaler":
        """æŒ‡å®šãƒ¢ãƒ‡ãƒ«ç”¨ã®Upscalerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—"""
        if cls._current_model_name != model_name:
            logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ: {cls._current_model_name} â†’ {model_name}")
            cls._cleanup_current()
            cls._global_instance = cls(model_name)
            cls._current_model_name = model_name
        return cls._global_instance
    
    @classmethod
    def force_cleanup(cls):
        """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†å‰ã®å¼·åˆ¶è§£æ”¾"""
        if cls._global_instance:
            logger.info(f"VRAMè§£æ”¾ã®ãŸã‚Upscalerå¼·åˆ¶ç ´æ£„: {cls._current_model_name}")
            cls._cleanup_current()
    
    @classmethod
    def _cleanup_current(cls):
        """ç¾åœ¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹è§£æ”¾"""
        if cls._global_instance:
            cls._global_instance._cleanup()
            cls._global_instance = None
            cls._current_model_name = None
            torch.cuda.empty_cache()  # GPU VRAMè§£æ”¾
```

#### Resource Management Features
- **Model Detection**: `models/upscalers/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªå‹•æ¤œå‡º
- **VRAM Management**: `torch.cuda.empty_cache()` ã«ã‚ˆã‚‹æ˜ç¤ºçš„GPU ãƒ¡ãƒ¢ãƒªè§£æ”¾
- **Error Handling**: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã®é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- **Logging**: ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆã¨ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ã®è©³ç´°ãƒ­ã‚°

### Integration Points

#### Batch Processing Integration
```python
# batch_processor.py ã§ã®ä½¿ç”¨ä¾‹
def process_directory_batch(..., upscaler_name: str | None = None):
    """åŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†ï¼ˆå…±æœ‰Upscalerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ï¼‰"""
    
    shared_upscaler = None
    if upscaler_name:
        shared_upscaler = Upscaler.get_for_model(upscaler_name)
        # â†‘ ãƒãƒƒãƒå…¨ä½“ã§åŒä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨
    
    for image_file in image_files:
        if shared_upscaler:
            # æ—¢ã«èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§é«˜é€Ÿå‡¦ç†
            result = shared_upscaler.upscale_image(image)
```

#### Annotation Service Integration
```python
# annotation_service.py ã§ã®ä½¿ç”¨ä¾‹
class AnnotationService:
    def start_annotation_batch(self, ...):
        """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†é–‹å§‹ï¼ˆVRAMç«¶åˆå›é¿ï¼‰"""
        
        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†å‰ã«Upscalerãƒªã‚½ãƒ¼ã‚¹å¼·åˆ¶è§£æ”¾
        Upscaler.force_cleanup()
        logger.info("ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†ã®ãŸã‚VRAMè§£æ”¾å®Œäº†")
        
        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼ˆVRAMãƒ•ãƒ«æ´»ç”¨å¯èƒ½ï¼‰
        results = self._execute_annotations(...)
```

### Performance Characteristics

#### Memory Usage
- **GPU VRAM**: 1ãƒ¢ãƒ‡ãƒ«åˆ†ã®ã¿ä¿æŒï¼ˆ4-6GBï¼‰
- **System RAM**: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼ˆ64MBç¨‹åº¦ï¼‰
- **Peak Usage**: ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«å®Ÿè¡Œæ™‚ã®ã¿

#### Processing Efficiency
- **Model Reuse**: åŒä¸€ãƒ¢ãƒ‡ãƒ«é€£ç¶šä½¿ç”¨æ™‚ã¯ç¬æ™‚å®Ÿè¡Œï¼ˆ0.1ç§’/ç”»åƒï¼‰
- **Model Switch**: ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆæ™‚ã®ã¿é…å»¶ï¼ˆ8-10ç§’ï¼‰
- **Batch Optimization**: 1000æšå‡¦ç†ã§99.9%ã®èª­ã¿è¾¼ã¿æ™‚é–“å‰Šæ¸›

#### VRAM Conflict Resolution
- **Problem**: ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ï¼ˆ6GBï¼‰+ ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ4GBï¼‰= 10GB â†’ OOM
- **Solution**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‰å¼·åˆ¶è§£æ”¾ â†’ VRAMç«¶åˆå›é¿
- **Trade-off**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¾Œåˆå›ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ã§å†èª­ã¿è¾¼ã¿é…å»¶

### Implementation Timeline

#### Phase 1: Core Module Implementation
1. `upscaler.py` - Upscalerã‚¯ãƒ©ã‚¹å˜ä½“å®Ÿè£…
2. `auto_crop.py` - AutoCropã‚¯ãƒ©ã‚¹åˆ†é›¢
3. `processing_manager.py` - çµ±åˆå‡¦ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

#### Phase 2: Resource Management Integration  
1. ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç®¡ç†å®Ÿè£…
2. å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½å®Ÿè£…
3. ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆæ¤œçŸ¥æ©Ÿèƒ½å®Ÿè£…

#### Phase 3: Service Integration
1. batch_processor.py ã¨ã®çµ±åˆ
2. annotation_service.py ã¨ã®çµ±åˆ
3. VRAMç«¶åˆå›é¿æ©Ÿèƒ½ã®å®Ÿè£…

### Decision Rationale

#### Rejected Alternative: Memory Caching
**æ¤œè¨æ¡ˆ**: ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã§state_dictã‚­ãƒ£ãƒƒã‚·ãƒ¥
**å´ä¸‹ç†ç”±**:
- å®Ÿè£…è¤‡é›‘æ€§å¢—åŠ ï¼ˆ2-3æ—¥ã®å·¥æ•°ï¼‰
- åŠ¹æœé™å®šçš„ï¼ˆ8ç§’â†’6ç§’ã€25%æ”¹å–„ã®ã¿ï¼‰
- ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è² å‚µï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
- ROIä¸é©åˆï¼ˆå°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¯¾ã—ã¦éå‰°ãªæœ€é©åŒ–ï¼‰

#### Selected Solution: Simple Force Cleanup
**é¸æŠç†ç”±**:
- å®Ÿè£…ã‚·ãƒ³ãƒ—ãƒ«ï¼ˆæ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ï¼‰
- é«˜ã„ä¿¡é ¼æ€§ï¼ˆå•é¡ŒãŒèµ·ãã«ãã„ï¼‰
- ä¿å®ˆæ€§å„ªç§€ï¼ˆç†è§£ã—ã‚„ã™ã„ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
- ååˆ†ãªåŠ¹æœï¼ˆVRAMç«¶åˆå®Œå…¨å›é¿ï¼‰

This technical specification provides comprehensive guidance for developing, maintaining, and deploying the LoRAIro application with focus on performance, security, and maintainability.